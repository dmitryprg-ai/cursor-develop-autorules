# ðŸ¤– Cursor AI Rules â€” Instruction System for AI Agents in Cursor IDE

<p align="center">
  <img src="https://img.shields.io/badge/version-2.0-blue" alt="Version">
  <img src="https://img.shields.io/badge/cursor-compatible-green" alt="Cursor Compatible">
  <img src="https://img.shields.io/badge/license-MIT-yellow" alt="License">
</p>

> **A modular system of rules and protocols to improve AI agent quality in Cursor IDE**

---

## ðŸŽ¯ What is this?

**Cursor AI Rules** is a ready-to-use instruction library for AI assistants in [Cursor IDE](https://cursor.com/) that:

- ðŸ“‹ **Structures AI work** â€” clear protocols for different task types
- ðŸ” **Improves quality** â€” built-in checks and result verification
- ðŸ“Š **Calibrates confidence** â€” AI honestly evaluates the reliability of its conclusions
- ðŸ”„ **Accumulates experience** â€” learning from errors to prevent repetition
- âœ… **Ensures verification** â€” Challenge protocol and Cross-check before completion

---

## ðŸ“ˆ What results will you get?

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Tasks without rework | ~50% | >80% | **+30%** |
| Repeated errors | ~30% | <10% | **-20%** |
| Linter errors on delivery | ~15% | 0% | **-15%** |
| Missed edge cases | frequent | rare | **â†“â†“â†“** |

### Real improvement examples:

**Before:**
```
AI: "Done! Component created."
Reality: File created but not opened. Has syntax errors. Linter not checked.
```

**After:**
```
AI: "Confidence: 95% â†’ 45% â€” code written, but not tested"
    â†’ Runs the code
    â†’ Cross-check completed
    â†’ Challenge protocol passed
    "Confidence: 92% â€” everything verified"
    
    âœ… COMPLETION REPORT
    Protocol: protocol-development.mdc
    Standards: standard-tdd.mdc
    Base Modules: _base-confidence, _base-challenge, _base-crosscheck
```

---

## ðŸ—ï¸ Architecture

```
.cursor/
â”œâ”€â”€ rules/                    # â­ Main instructions (21 files)
â”‚   â”œâ”€â”€ core-master.mdc       # Single entry point (alwaysApply: true)
â”‚   â”œâ”€â”€ _base-*.mdc           # Base modules (7 pcs)
â”‚   â”œâ”€â”€ protocol-*.mdc        # Task-type protocols (7 pcs)
â”‚   â”œâ”€â”€ standard-*.mdc        # Quality standards (5 pcs)
â”‚   â””â”€â”€ error-learning.mdc    # Error learning
â”‚
â””â”€â”€ rules_alone/              # ðŸŽ¯ Standalone instructions (4 files)
    â””â”€â”€ *.mdc                 # Called explicitly via @
```

### How it works:

```
Your request
    â†“
core-master.mdc (determines complexity and type)
    â†“
protocol-*.mdc (executes the task)
    â†“
_base-*.mdc (applies checks)
    â†“
standard-*.mdc (verifies quality)
    â†“
âœ… COMPLETION REPORT
```

---

## ðŸš€ Quick Start

### Step 1: Copy the folder

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/cursor-ai-rules.git

# Copy .cursor to your project
cp -r cursor-ai-rules/.cursor /path/to/your/project/
```

### Step 2: Done!

Just start working in Cursor IDE. Instructions are applied **automatically**.

```
Add a component to display statistics
```

AI will automatically:
1. Determine task complexity (Simple/Standard/Complex)
2. Choose the appropriate protocol
3. Apply checks and verifications
4. Output an execution report

---

## ðŸ“‹ Library Contents

### ðŸ”„ Protocols (7 items)

| Protocol | When to apply |
|----------|---------------|
| `protocol-prepare-prompt` | Improving user prompt before execution |
| `protocol-development` | Developing new features |
| `protocol-bugfix` | Fixing bugs |
| `protocol-refactoring` | Code refactoring |
| `protocol-research` | Data analysis (parquet, csv, SQL) |
| `protocol-freeze-recovery` | Recovery after AI freeze |
| `protocol-session-review` | Session analysis and improvement accumulation |

### ðŸ“¦ Base Modules (7 items)

| Module | What it does |
|--------|--------------|
| `_base-confidence` | AI confidence calibration (deduction formula) |
| `_base-challenge` | 4 questions before "done" |
| `_base-crosscheck` | Independent result verification |
| `_base-forbidden` | Critical prohibitions |
| `_base-todo-usage` | TODO usage rules |
| `_base-5wh` | 5W+H format for analysis |
| `_base-jtbd-thinking` | JTBD thinking for user-facing features |

### ðŸ“‹ Quality Standards (5 items)

| Standard | Purpose |
|----------|---------|
| `standard-agent-quality` | Success metrics and agent boundaries |
| `standard-qa` | QA acceptance criteria |
| `standard-rca` | Root Cause Analysis (5 Whys) |
| `standard-tdd` | Test-Driven Development |
| `standard-cto-review` | CTO/Lead Review for complex tasks |

### ðŸŽ¯ Standalone Instructions (4 items)

Called explicitly via `@`:

```
@rules_alone/core-duplicate-check Check for duplicates before creating
@rules_alone/ajtbd-evaluation Evaluate the landing page
@rules_alone/backlog-to-rules Implement improvements from backlog
```

---

## ðŸ’¡ Key Concepts

### 1. Confidence Calibration

AI honestly evaluates the reliability of its conclusions:

```
Confidence = 100% minus:
â€¢ Code not tested: -50%
â€¢ Artifacts not opened: -40%
â€¢ No cross-check: -30%
â€¢ Assumptions not verified: -25%

Threshold: <80% = not ready
```

### 2. Challenge Protocol

Before each "done", AI asks itself 4 questions:

1. How can I disprove my conclusion?
2. Did I open ALL created files?
3. What edge cases did I miss?
4. Does this solve the user's real Job?

### 3. Cross-check

Verifying the result using a **different method**:

- File created â†’ Open and read it
- API works â†’ curl + code
- Data is correct â†’ pandas + SQL

### 4. COMPLETION REPORT

At the end of each task, AI outputs a report:

```markdown
## âœ… COMPLETION REPORT

**Complexity:** ðŸŸ¡ STANDARD

**Instructions used:**
- Protocol: protocol-development.mdc
- Standards: standard-tdd.mdc
- Base Modules: _base-confidence, _base-challenge, _base-crosscheck

**Final confidence:** 92%
```

---

## ðŸ”§ Project Customization

### Add a file for accumulating improvements

Create in project root:

```markdown
# {projectname}-improvements-backlog.md

## ðŸ“Š Statistics
- Total improvements: 0
- Implemented: 0

## ðŸ“ Backlog
(entries will be added automatically)
```

### Configure project-specific checks

If you have project-specific checks, add them to `protocol-development.mdc`:

```markdown
### X.Y. [Your check]

**Input:** [When to apply]

**Output:** [What should be done]

**WHY:** [Real error case]
```

---

## ðŸ“Š Task Complexity Definition

| Complexity | Signs | Flow |
|------------|-------|------|
| ðŸŸ¢ SIMPLE | 1-2 files, obvious result | EXECUTE â†’ VERIFY |
| ðŸŸ¡ STANDARD | New functionality, multiple files | Full 4-phase protocol |
| ðŸ”´ COMPLEX | Architecture, critical data | + CTO Review + Session Review |

---

## â“ FAQ

### Do I need to write "use core-master.mdc"?
**No.** Files with `alwaysApply: true` are applied automatically.

### Can I use this with other AI assistants?
The library is optimized for **Cursor IDE**, but the concepts are universal.

### How to add my own rules?
1. Create a file `protocol-your-name.mdc` or `_base-your-name.mdc`
2. Add a reference in `core-master.mdc`

### How to disable instructions?
Change `alwaysApply: true` to `false` in `core-master.mdc`.

---

## ðŸ¤ Contributing

Welcome:
- ðŸ› Bug reports
- ðŸ’¡ Improvement suggestions
- ðŸ“ New protocols and modules

---

## ðŸ“„ License

MIT License â€” use freely in any projects.

---

## ðŸ”— Links

- [Cursor IDE](https://cursor.com/)
- [Cursor Docs â€” Large Codebases](https://cursor.com/docs/cookbook/large-codebases)

---

**Version:** 2.0  
**Date:** 2026-01-09

