# ğŸ¤– Cursor AI Rules â€” Instruction System for AI Agents in Cursor IDE

<p align="center">
  <img src="https://img.shields.io/badge/version-3.1-blue" alt="Version">
  <img src="https://img.shields.io/badge/cursor-compatible-green" alt="Cursor Compatible">
  <img src="https://img.shields.io/badge/license-MIT-yellow" alt="License">
</p>

> **A modular system of rules and protocols to improve AI agent quality in Cursor IDE**

---

## ğŸ¯ What is this?

**Cursor AI Rules** is a ready-to-use instruction library for AI assistants in [Cursor IDE](https://cursor.com/) that:

- ğŸ“‹ **Structures AI work** â€” clear protocols for different task types
- ğŸ” **Improves quality** â€” built-in checks and result verification
- ğŸ“Š **Calibrates confidence** â€” AI honestly evaluates the reliability of its conclusions
- ğŸ”„ **Accumulates experience** â€” learning from errors to prevent repetition
- âœ… **Ensures verification** â€” Challenge protocol and Cross-check before completion

---

## ğŸ“ˆ What results will you get?

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Tasks without rework | ~50% | >80% | **+30%** |
| Repeated errors | ~30% | <10% | **-20%** |
| Linter errors on delivery | ~15% | 0% | **-15%** |
| Missed edge cases | frequent | rare | **â†“â†“â†“** |

### Real improvement examples:

**Before:**
```
AI: "Done! Component created."
Reality: File created but not opened. Has syntax errors. Linter not checked.
```

**After:**
```
AI: "Confidence: 95% â†’ 45% â€” code written, but not tested"
    â†’ Runs the code
    â†’ Cross-check completed
    â†’ Challenge protocol passed
    "Confidence: 92% â€” everything verified"
    
    âœ… COMPLETION REPORT
    Protocol: protocol-development.mdc
    Standards: standard-tdd.mdc
    Base Modules: _base-confidence, _base-challenge, _base-crosscheck
```

---

## ğŸ—ï¸ Architecture

```
your-project/
â”œâ”€â”€ .cursor/                       # â­ Universal instructions
â”‚   â”œâ”€â”€ CHANGELOG.md               # Change history
â”‚   â”œâ”€â”€ rules/                     # Main instructions (23 files)
â”‚   â”‚   â”œâ”€â”€ core-master.mdc        # Single entry point (alwaysApply: true)
â”‚   â”‚   â”œâ”€â”€ _base-*.mdc            # Base modules (8 pcs)
â”‚   â”‚   â”œâ”€â”€ protocol-*.mdc         # Task-type protocols (7 pcs)
â”‚   â”‚   â”œâ”€â”€ standard-*.mdc         # Quality standards (5 pcs)
â”‚   â”‚   â””â”€â”€ error-learning.mdc     # Error learning
â”‚   â”‚
â”‚   â””â”€â”€ rules_alone/               # Standalone instructions (4 files)
â”‚       â””â”€â”€ *.mdc                  # Called explicitly via @
â”‚
â”œâ”€â”€ .cursor_additional/            # ğŸ“ Project-specific files
â”‚   â””â”€â”€ {projectname}/             # Folder for your project
â”‚       â”œâ”€â”€ improvements-backlog.md # Improvement accumulation
â”‚       â””â”€â”€ error-log.md           # Error logs
â”‚
â””â”€â”€ AGENTS.md                      # Quick Start for AI agents
```

### How it works:

```
Your request
    â†“
core-master.mdc (determines complexity and type)
    â†“
protocol-*.mdc (executes the task)
    â†“
_base-*.mdc (applies checks)
    â†“
standard-*.mdc (verifies quality)
    â†“
âœ… COMPLETION REPORT
```

---

## ğŸš€ Quick Start

### Step 1: Copy files to your project

```bash
# Clone the repository
git clone https://github.com/dmitryprg-ai/cursor-develop-autorules.git

# Copy .cursor and AGENTS.md to your project
cp -r cursor-develop-autorules/.cursor /path/to/your/project/
cp cursor-develop-autorules/AGENTS.md /path/to/your/project/
```

### Step 2: Done!

Just start working in Cursor IDE. Instructions are applied **automatically**.

```
Add a component to display statistics
```

AI will automatically:
1. Determine task complexity (Simple/Standard/Complex)
2. Choose the appropriate protocol
3. Apply checks and verifications
4. Output an execution report

---

## ğŸ“‹ Library Contents

### ğŸ”„ Protocols (7 items)

| Protocol | When to apply |
|----------|---------------|
| `protocol-prepare-prompt` | Improving user prompt before execution |
| `protocol-development` | Developing new features |
| `protocol-bugfix` | Fixing bugs |
| `protocol-refactoring` | Code refactoring |
| `protocol-research` | Data analysis (parquet, csv, SQL) |
| `protocol-freeze-recovery` | Recovery after AI freeze |
| `protocol-session-review` | Session analysis and improvement accumulation |

### ğŸ“¦ Base Modules (8 items)

| Module | What it does |
|--------|--------------|
| `_base-confidence` | AI confidence calibration (deduction formula) |
| `_base-challenge` | 4 questions before "done" |
| `_base-crosscheck` | Independent result verification |
| `_base-forbidden` | Critical prohibitions |
| `_base-todo-usage` | TODO usage rules |
| `_base-5wh` | 5W+H format for analysis |
| `_base-jtbd-thinking` | JTBD thinking for user-facing features |
| `_base-rat` | **NEW:** Riskiest Assumption Test â€” verify risks BEFORE implementation |

### ğŸ“‹ Quality Standards (5 items)

| Standard | Purpose |
|----------|---------|
| `standard-agent-quality` | Success metrics and agent boundaries |
| `standard-qa` | QA acceptance criteria |
| `standard-rca` | Root Cause Analysis (5 Whys) |
| `standard-tdd` | Test-Driven Development |
| `standard-cto-review` | CTO/Lead Review for complex tasks |

### ğŸ¯ Standalone Instructions (4 items)

Called explicitly via `@`:

```
@rules_alone/core-duplicate-check Check for duplicates before creating
@rules_alone/ajtbd-evaluation Evaluate the landing page
@rules_alone/backlog-to-rules Implement improvements from backlog
```

---

## ğŸ’¡ Key Concepts

### 0. RAT â€” Riskiest Assumption Test

Verify risky assumptions **BEFORE** starting implementation:

```
RAT = 3 steps:
1. List ALL assumptions
2. Rank by risk (what can "kill" the solution)
3. Verify TOP-1 risk BEFORE coding

If risk is disproved â†’ revise the plan!
```

**Typical coding risks:**
- ğŸ”§ Does the approach/library fit?
- ğŸ“Š Is data/API as expected?
- ğŸ”— Won't break existing code?

> Source: [Ivan Zamesin â€” RAT](https://zamesin.ru/books/product-howto/riskiest-assumption-test/)

### 1. Confidence Calibration

AI honestly evaluates the reliability of its conclusions:

```
Confidence = 100% minus:
â€¢ Code not tested: -50%
â€¢ Artifacts not opened: -40%
â€¢ No cross-check: -30%
â€¢ Assumptions not verified: -25%

Threshold: <80% = not ready
```

### 2. Challenge Protocol

Before each "done", AI asks itself 4 questions:

1. How can I disprove my conclusion?
2. Did I open ALL created files?
3. What edge cases did I miss?
4. Does this solve the user's real Job?

### 3. Cross-check

Verifying the result using a **different method**:

- File created â†’ Open and read it
- API works â†’ curl + code
- Data is correct â†’ pandas + SQL

### 4. COMPLETION REPORT

At the end of each task, AI outputs a report:

```markdown
## âœ… COMPLETION REPORT

**Complexity:** ğŸŸ¡ STANDARD

**Instructions used:**
- Protocol: protocol-development.mdc
- Standards: standard-tdd.mdc
- Base Modules: _base-confidence, _base-challenge, _base-crosscheck

**Final confidence:** 92%
```

---

## ğŸ”§ Project Customization

### Configure AGENTS.md

After copying, edit `AGENTS.md` for your project:
- Specify project structure
- Add build commands
- Describe code style

### Add project-specific checks

If you have project-specific checks, add them to `protocol-development.mdc`:

```markdown
### X.Y. [Your check]

**Input:** [When to apply]

**Output:** [What should be done]

**WHY:** [Real error case]
```

---

## ğŸ”„ Recording Failures and Self-Improvement

The system includes an error learning mechanism. When AI makes a mistake or the user points out a problem, it gets recorded and turned into new rules.

### How it works

```
Error detected
        â†“
Session Review (protocol-session-review.mdc)
        â†“
Record in improvements-backlog.md
        â†“
Implement via @rules_alone/backlog-to-rules
        â†“
New rule in instructions
        â†“
Error doesn't repeat âœ…
```

### Step 1: Create a file for accumulating improvements

Create a folder and file for your project:

```bash
mkdir -p .cursor_additional/{projectname}/
```

Create file `.cursor_additional/{projectname}/improvements-backlog.md`:

```markdown
# ğŸ“‹ IMPROVEMENTS BACKLOG

> **Project:** {projectname}

## ğŸ“Š STATISTICS
| Metric | Value |
|--------|-------|
| Total improvements | 0 |
| ğŸ”´ High priority | 0 |
| âœ… Implemented | 0 |

## ğŸ”´ HIGH PRIORITY
(entries will go here)

## âœ… IMPLEMENTED
(implemented improvements will go here)
```

### Step 2: Record errors after failures

When AI makes a mistake, record in backlog:

```markdown
---

### IMPROVEMENT #N: YYYY-MM-DD (Brief name)

**Source:** Session Review after [which task]

**Problem:**
[What went wrong]

**Root Cause:**
[Why it happened â€” 5 Whys if needed]

**Proposed change:**
```
[Specific text to add to instructions]
```

**File to modify:** `.cursor/rules/[file].mdc`

**Priority:** ğŸ”´ High / ğŸŸ¡ Medium / ğŸŸ¢ Low
**Status:** ğŸ“ Backlog

---
```

### Step 3: Implement improvements

When 2+ High priority improvements accumulate or a week passes:

```
@rules_alone/backlog-to-rules Implement accumulated improvements
```

AI will automatically:
1. Read the backlog
2. Group improvements by files
3. Add new sections to instructions
4. Update statuses in backlog (ğŸ“ â†’ âœ…)
5. Output a report

### Real improvement example

**Was:** AI created `.cursor` inside `git/`, although user said "I'll copy 2 folders myself"

**Recorded in backlog:**
```markdown
### IMPROVEMENT #11: Literal Request Following

**Problem:** AI interprets request instead of following literally

**Root Cause:** No explicit step to "write constraints verbatim"

**Proposed change:** Add Explicit Constraints section to protocol-prepare-prompt.mdc
```

**After implementation:** Now AI always writes explicit constraints from the request verbatim.

### When to run implementation

| Condition | Priority |
|-----------|----------|
| 5+ improvements accumulated | ğŸ”´ Required |
| 2+ High priority exist | ğŸ”´ Required |
| Week passed | ğŸŸ¡ Recommended |
| Same error repeated 3+ times | ğŸ”´ Immediately |

---

## ğŸ“Š Task Complexity Definition

| Complexity | Signs | Flow |
|------------|-------|------|
| ğŸŸ¢ SIMPLE | 1-2 files, obvious result | EXECUTE â†’ VERIFY |
| ğŸŸ¡ STANDARD | New functionality, multiple files | Full 4-phase protocol |
| ğŸ”´ COMPLEX | Architecture, critical data | + CTO Review + Session Review |

---

## â“ FAQ

### Do I need to write "use core-master.mdc"?
**No.** Files with `alwaysApply: true` are applied automatically.

### Can I use this with other AI assistants?
The library is optimized for **Cursor IDE**, but the concepts are universal.

### How to add my own rules?
1. Create a file `protocol-your-name.mdc` or `_base-your-name.mdc`
2. Add a reference in `core-master.mdc`

### How to disable instructions?
Change `alwaysApply: true` to `false` in `core-master.mdc`.

---

## ğŸ¤ Contributing

Welcome:
- ğŸ› Bug reports
- ğŸ’¡ Improvement suggestions
- ğŸ“ New protocols and modules

---

## ğŸ“„ License

MIT License â€” use freely in any projects.

---

## ğŸ”— Links

- [GitHub Repository](https://github.com/dmitryprg-ai/cursor-develop-autorules)
- [Cursor IDE](https://cursor.com/)
- [Cursor Docs â€” Large Codebases](https://cursor.com/docs/cookbook/large-codebases)

---

**Version:** 3.1  
**Date:** 2026-01-10

