---
description: Methodology for validation from the end - define expected result first, then verify achievement
alwaysApply: false
---
# üéØ FROM-THE-END VALIDATION METHODOLOGY

## Principle

**Start with the END in mind:**
1. Define expected OUTPUT before starting work
2. Create test cases that verify the OUTPUT
3. Work backwards from OUTPUT to implementation
4. Verify OUTPUT matches expectation

## 1. Before Starting Any Task

```markdown
## üéØ EXPECTED OUTPUT DEFINITION

### Task: [What needs to be done]

### Expected Output:
- [ ] [Concrete deliverable 1]
- [ ] [Concrete deliverable 2]
- [ ] [Concrete deliverable 3]

### Success Criteria:
1. [Measurable criterion 1]
2. [Measurable criterion 2]
3. [Measurable criterion 3]

### Test Cases:
| # | Input | Expected Output | Pass/Fail |
|---|-------|-----------------|-----------|
| 1 | [input] | [expected] | [ ] |
| 2 | [input] | [expected] | [ ] |
| 3 | [negative case] | [expected error] | [ ] |

### Artifacts to Produce:
- [ ] [file/component/function]
- [ ] [documentation]
- [ ] [test]
```

## 2. Output Quality Checklist

**Before declaring "done", verify:**

```markdown
## ‚úÖ OUTPUT QUALITY CHECKLIST

### Functional Requirements:
- [ ] Does output match expected result?
- [ ] Do all test cases pass?
- [ ] Are edge cases handled?
- [ ] Are errors handled gracefully?

### Technical Requirements:
- [ ] Code compiles/builds without errors?
- [ ] No linter warnings/errors?
- [ ] Tests pass?
- [ ] No regressions introduced?

### Documentation:
- [ ] Is output clear to user/developer?
- [ ] Are limitations documented?
- [ ] Are next steps clear?

### Cross-Check:
- [ ] Verified output independently?
- [ ] Opened and checked each artifact?
- [ ] Compared expected vs actual?
```

## 3. Gap Analysis Template

```markdown
## üîç GAP ANALYSIS: Expected vs Actual

### Expected Output:
[What was supposed to be delivered]

### Actual Output:
[What was actually delivered]

### Gaps Found:
| # | Expected | Actual | Gap | Action |
|---|----------|--------|-----|--------|
| 1 | [expected] | [actual] | [difference] | [fix] |
| 2 | [expected] | [actual] | [difference] | [fix] |

### Root Cause of Gaps:
- [Why gap 1 exists]
- [Why gap 2 exists]

### Remediation Plan:
1. [Action to close gap 1]
2. [Action to close gap 2]
```

## 4. Verification Protocol

**After completing work:**

1. **Open each artifact** - physically read the file
2. **Run each test case** - don't assume they pass
3. **Compare expected vs actual** - document differences
4. **Cross-check independently** - use different method to verify

```markdown
## üîç VERIFICATION REPORT

### Artifacts Verified:
| Artifact | Opened | Checked | Status |
|----------|--------|---------|--------|
| [file1] | ‚úÖ | ‚úÖ | OK |
| [file2] | ‚úÖ | ‚úÖ | ISSUE |

### Test Cases Executed:
| Test | Expected | Actual | Pass |
|------|----------|--------|------|
| [test1] | [exp] | [act] | ‚úÖ |
| [test2] | [exp] | [act] | ‚ùå |

### Issues Found:
- [Issue 1 with details]
- [Issue 2 with details]

### Verification Methods Used:
1. [Method 1: e.g., manual file inspection]
2. [Method 2: e.g., API call test]
3. [Method 3: e.g., UI verification]
```

## 5. Anti-Patterns

‚ùå **NEVER:**
- Say "done" without defining what "done" means
- Skip test case execution
- Assume output is correct without checking
- Close task without gap analysis
- Declare success without independent verification

‚úÖ **ALWAYS:**
- Define expected output FIRST
- Write test cases BEFORE implementation
- Open and read every artifact
- Compare expected vs actual
- Document any gaps found

## 6. Quick Reference

```
BEFORE: What should the output be?
DURING: Am I moving toward that output?
AFTER:  Does output match expectation?
ALWAYS: Can I prove it with evidence?
```
