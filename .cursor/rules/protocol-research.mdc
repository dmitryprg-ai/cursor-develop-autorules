---
description: –ü—Ä–æ—Ç–æ–∫–æ–ª –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö ‚Äî structured approach to data analysis
alwaysApply: false
---
# üìä PROTOCOL: DATA RESEARCH

> **–ê–∫—Ç–∏–≤–∏—Ä—É–µ—Ç—Å—è —á–µ—Ä–µ–∑:** `core-master.mdc` –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–∞–Ω–Ω—ã–º–∏
> **–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:** –¥–∞–Ω–Ω—ã–µ, parquet, csv, –∞–Ω–∞–ª–∏–∑, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, pandas, dataframe
> **–ë–∞–∑–æ–≤—ã–µ –º–æ–¥—É–ª–∏:** `_base-confidence.mdc`, `_base-5wh.mdc`, `_base-challenge.mdc`

---

## Core Principle: DATA FIRST, CODE SECOND

```
1. LOAD   ‚Üí –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ, –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
2. SCHEMA ‚Üí –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É (types, shape, samples)
3. PROFILE ‚Üí –ù–∞–π—Ç–∏ —Ä–∏—Å–∫–∏ (nulls, duplicates, anomalies)
4. HYPOTHESIS ‚Üí –ß—Ç–æ —Ö–æ—Ç–∏–º –¥–æ–∫–∞–∑–∞—Ç—å?
5. EXPERIMENT ‚Üí –û–¥–∏–Ω –º–∞–ª–µ–Ω—å–∫–∏–π —Ç–µ—Å—Ç
6. DOCUMENT ‚Üí –ó–∞–ø–∏—Å–∞—Ç—å –Ω–∞—Ö–æ–¥–∫–∏ –ø–æ 5W+H
```

---

## 1. DATA LOADING PROTOCOL

### –ü–µ—Ä–µ–¥ –õ–Æ–ë–û–ô —Ä–∞–±–æ—Ç–æ–π —Å –¥–∞–Ω–Ω—ã–º–∏:

```python
# –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û: –ü—Ä–æ–≤–µ—Ä—å —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –∏ –¥–æ—Å—Ç—É–ø–Ω—ã
import os
file_path = "path/to/data.parquet"
assert os.path.exists(file_path), f"File not found: {file_path}"
print(f"File size: {os.path.getsize(file_path) / 1024 / 1024:.2f} MB")
```

### Checklist –∑–∞–≥—Ä—É–∑–∫–∏:
- [ ] –§–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –¥–æ—Å—Ç—É–ø–µ–Ω
- [ ] –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ —Ä–∞–∑—É–º–Ω—ã–π
- [ ] –ö–æ–¥–∏—Ä–æ–≤–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è (UTF-8, cp1251, etc.)
- [ ] –ù–µ—Ç permission errors

---

## 2. SCHEMA ANALYSIS (MANDATORY)

### –í–°–ï–ì–î–ê –ø–æ–∫–∞–∑—ã–≤–∞–π –ø–µ—Ä–µ–¥ –≤—ã–≤–æ–¥–∞–º–∏:

```python
import pandas as pd

# Load data
df = pd.read_parquet("data.parquet")

# MANDATORY OUTPUT:
print("=" * 60)
print("SCHEMA ANALYSIS")
print("=" * 60)
print(f"Shape: {df.shape}")
print(f"\nData Types:\n{df.dtypes}")
print(f"\nSample (head 5):\n{df.head()}")
print(f"\nUnique counts:\n{df.nunique()}")
print(f"\nNull counts:\n{df.isnull().sum()}")
print("=" * 60)
```

### Schema Report Template:

```markdown
## üìã SCHEMA REPORT

### File: [path]
### Shape: [rows] x [columns]

### Columns:
| Column | Type | Nulls | Uniques | Sample Values |
|--------|------|-------|---------|---------------|
| col1 | int64 | 0 | 100 | 1, 2, 3... |
| col2 | object | 5 | 50 | "A", "B"... |

### Observations:
- [observation 1]
- [observation 2]
```

---

## 3. RISK PROFILING

### –ü—Ä–æ–≤–µ—Ä—å —Ç–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –¥–∞–Ω–Ω—ã—Ö:

```python
# Nulls
print("NULLS:")
print(df.isnull().sum())

# Duplicates
print(f"\nDUPLICATES: {df.duplicated().sum()}")

# Data types issues
print("\nPOTENTIAL TYPE ISSUES:")
for col in df.select_dtypes(include=['object']).columns:
    sample = df[col].dropna().head(3).tolist()
    print(f"  {col}: {sample}")
```

### Risk Categories:

| Risk | Check | Action |
|------|-------|--------|
| Missing data | `df.isnull().sum()` | Document, decide handling |
| Duplicates | `df.duplicated().sum()` | Investigate, dedupe if needed |
| Wrong types | Manual inspection | Convert types |
| Outliers | `df.describe()` | Investigate, document |
| Encoding | Try reading | Fix encoding |

---

## 4. HYPOTHESIS FORMULATION

### –ü–µ—Ä–µ–¥ –ª—é–±—ã–º –∞–Ω–∞–ª–∏–∑–æ–º:

```markdown
## üéØ HYPOTHESIS

### Question:
[–ß—Ç–æ —Ö–æ—Ç–∏–º —É–∑–Ω–∞—Ç—å/–¥–æ–∫–∞–∑–∞—Ç—å?]

### Expected outcome:
[–ß—Ç–æ –æ–∂–∏–¥–∞–µ–º –Ω–∞–π—Ç–∏]

### Success criteria:
- [Criterion 1]
- [Criterion 2]

### Assumptions:
- [Assumption 1]
- [Assumption 2]
```

---

## 5. MINI-EXPERIMENT PROTOCOL

### –û–¥–∏–Ω –º–∞–ª–µ–Ω—å–∫–∏–π —Ç–µ—Å—Ç –∑–∞ —Ä–∞–∑:

```python
# EXPERIMENT: [Description]
# HYPOTHESIS: [What we expect]

# Step 1: Minimal query
result = df[df['column'] == 'value'].shape[0]

# Step 2: Output result
print(f"Result: {result}")
print(f"Expected: [expected value]")
print(f"Status: {'‚úÖ PASS' if result == expected else '‚ùå FAIL'}")
```

### –ü—Ä–∞–≤–∏–ª–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞:
- [ ] –û–¥–∏–Ω –≤–æ–ø—Ä–æ—Å per experiment
- [ ] –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π (same input ‚Üí same output)
- [ ] –ë—ã—Å—Ç—Ä—ã–π (< 30 —Å–µ–∫—É–Ω–¥)
- [ ] Logged (print results)
- [ ] –°—Ä–∞–≤–Ω—ë–Ω —Å expectation

---

## 6. DOCUMENTATION (5W+H Format)

### –ö–∞–∂–¥–∞—è –Ω–∞—Ö–æ–¥–∫–∞ –¥–æ–ª–∂–Ω–∞ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞:

–ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç –∏–∑ `_base-5wh.mdc`:

| Question | Answer |
|----------|--------|
| **What** | –ß—Ç–æ –Ω–∞–π–¥–µ–Ω–æ? (–∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ, —Å —á–∏—Å–ª–∞–º–∏) |
| **When** | –ö–æ–≥–¥–∞ —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç? (period, conditions) |
| **Where** | –ì–¥–µ –≤ –¥–∞–Ω–Ω—ã—Ö? (file, column, row range) |
| **Who** | –ö—Ç–æ/—á—Ç–æ –∑–∞—Ç—Ä–æ–Ω—É—Ç–æ? (records, users) |
| **Why** | –ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ? (impact) |
| **How** | –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å? (next steps) |

---

## 7. COGNITIVE BIAS PREVENTION

### üö® –ò–ó–ë–ï–ì–ê–ô:

#### Survivorship Bias:
- ‚ùå –ê–Ω–∞–ª–∏–∑ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã—Ö N –∑–∞–ø–∏—Å–µ–π
- ‚ùå –°–∫—Ä—ã—Ç—ã–µ –ª–∏–º–∏—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
- ‚ùå –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è "—É—Å–ø–µ—à–Ω—ã—Ö" –∑–∞–ø–∏—Å–µ–π
- ‚úÖ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –í–°–ï –¥–∞–Ω–Ω—ã–µ

#### Confirmation Bias:
- ‚ùå –ò—Å–∫–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –≥–∏–ø–æ—Ç–µ–∑—ã
- ‚ùå –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏
- ‚úÖ –ò—Å–∫–∞—Ç—å –û–ü–†–û–í–ï–†–ñ–ï–ù–ò–Ø –≥–∏–ø–æ—Ç–µ–∑—ã
- ‚úÖ –ü—Ä–æ–≤–µ—Ä—è—Ç—å –≤ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö

#### False Mental Model:
- ‚ùå –í—ã–≤–æ–¥—ã –Ω–∞ –Ω–µ–ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- ‚ùå –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ anomalies
- ‚úÖ Edge cases —Ç–æ–∂–µ –¥–∞–Ω–Ω—ã–µ
- ‚úÖ –û—à–∏–±–∫–∏ ‚Äî –≤–∞–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è

---

## 8. RESEARCH SESSION CHECKLIST

```markdown
## ‚úÖ RESEARCH SESSION CHECKLIST

### Data Loading:
- [ ] File exists and accessible
- [ ] Size checked
- [ ] Encoding verified

### Schema Analysis:
- [ ] Shape printed
- [ ] dtypes shown
- [ ] head() displayed
- [ ] nunique() checked
- [ ] nulls counted

### Risk Profiling:
- [ ] Duplicates checked
- [ ] Nulls analyzed
- [ ] Types verified
- [ ] Outliers identified

### Hypothesis:
- [ ] Question formulated
- [ ] Expected outcome stated
- [ ] Success criteria defined

### Experiment:
- [ ] One question tested
- [ ] Result logged
- [ ] Compared to expectation

### Documentation:
- [ ] 5W+H answered
- [ ] Evidence provided
- [ ] Recommendations made
```

---

## 9. CONFIDENCE CALIBRATION

–ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª—É –∏–∑ `_base-confidence.mdc`:

```
Base: 100%
- Schema –Ω–µ –ø–æ–∫–∞–∑–∞–Ω: -40%
- –ù–µ—Ç head() sample: -30%
- –ü—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö: -50%
- –ë–æ–ª—å—à–æ–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (>1000 rows first try): -20%
- 5W+H –Ω–µ –æ—Ç–≤–µ—á–µ–Ω: -25%
```

---

## 10. ANTI-PATTERNS (AVOID)

### ‚ùå –í—ã–≤–æ–¥—ã –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö:
```
WRONG: "–î–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç..."
RIGHT: "df.shape –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç 1000 rows, df['col'].nunique() –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç 50 unique values"
```

### ‚ùå Skip schema analysis:
```
WRONG: –ù–∞—á–∞—Ç—å –∫–æ–¥–∏—Ç—å —Å—Ä–∞–∑—É
RIGHT: –í—Å–µ–≥–¥–∞ –ø–æ–∫–∞–∑–∞—Ç—å dtypes, head(), nunique() —Å–Ω–∞—á–∞–ª–∞
```

### ‚ùå Large experiments:
```
WRONG: –°–ª–æ–∂–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö —Å—Ä–∞–∑—É
RIGHT: –ú–∞–ª–µ–Ω—å–∫–∏–π –∑–∞–ø—Ä–æ—Å, limited rows, –æ–¥–∏–Ω –≤–æ–ø—Ä–æ—Å
```

---
**–í–µ—Ä—Å–∏—è:** 1.0
**–°–≤—è–∑–∞–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏:** `_base-5wh.mdc`, `_base-confidence.mdc`, `_base-challenge.mdc`
