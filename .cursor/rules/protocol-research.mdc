---
description: "–ü—Ä–∏–º–µ–Ω—è—Ç—å –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–∞–Ω–Ω—ã–º–∏ –∏ –∞–Ω–∞–ª–∏–∑–µ. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: –¥–∞–Ω–Ω—ã–µ, parquet, csv, –∞–Ω–∞–ª–∏–∑, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, pandas. –¢—Ä–µ–±—É–µ—Ç: —Å–Ω–∞—á–∞–ª–∞ SCHEMA –ø–æ–∫–∞–∑–∞—Ç—å (dtypes, head, nunique), –∑–∞—Ç–µ–º hypothesis, –∑–∞—Ç–µ–º mini-experiment, –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ 5W+H."
globs: 
alwaysApply: false
tags: [data, analysis, research, pandas]
---

# üìä PROTOCOL: DATA RESEARCH

## Context

- **–ö–æ–≥–¥–∞:** –†–∞–±–æ—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏, –∞–Ω–∞–ª–∏–∑, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
- **–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:** –¥–∞–Ω–Ω—ã–µ, parquet, csv, –∞–Ω–∞–ª–∏–∑, pandas, dataframe
- **–ü—Ä–∏–Ω—Ü–∏–ø:** DATA FIRST, CODE SECOND

---

## Requirements

### Workflow

<required>
  1. LOAD ‚Üí –ó–∞–≥—Ä—É–∑–∏—Ç—å, –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
  2. SCHEMA ‚Üí –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É (types, shape, samples)
  3. PROFILE ‚Üí –ù–∞–π—Ç–∏ —Ä–∏—Å–∫–∏ (nulls, duplicates, anomalies)
  4. HYPOTHESIS ‚Üí –ß—Ç–æ —Ö–æ—Ç–∏–º –¥–æ–∫–∞–∑–∞—Ç—å?
  5. EXPERIMENT ‚Üí –û–¥–∏–Ω –º–∞–ª–µ–Ω—å–∫–∏–π —Ç–µ—Å—Ç
  6. DOCUMENT ‚Üí –ó–∞–ø–∏—Å–∞—Ç—å –ø–æ 5W+H
</required>

### Schema Analysis (MANDATORY)

<critical>
  –í–°–ï–ì–î–ê –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø–µ—Ä–µ–¥ –≤—ã–≤–æ–¥–∞–º–∏:
  ```python
  print(f"Shape: {df.shape}")
  print(f"dtypes:\n{df.dtypes}")
  print(f"head:\n{df.head()}")
  print(f"nunique:\n{df.nunique()}")
  print(f"nulls:\n{df.isnull().sum()}")
  ```
</critical>

### Risk Profiling

| Risk | Check | Action |
|------|-------|--------|
| Missing data | `df.isnull().sum()` | Document, decide handling |
| Duplicates | `df.duplicated().sum()` | Investigate |
| Wrong types | Manual inspection | Convert types |
| Outliers | `df.describe()` | Investigate |

### Mini-Experiment Protocol

```python
# EXPERIMENT: [Description]
# HYPOTHESIS: [What we expect]

result = df[df['column'] == 'value'].shape[0]

print(f"Result: {result}")
print(f"Expected: {expected}")
print(f"Status: {'‚úÖ PASS' if result == expected else '‚ùå FAIL'}")
```

<required>
  - –û–¥–∏–Ω –≤–æ–ø—Ä–æ—Å per experiment
  - –ë—ã—Å—Ç—Ä—ã–π (< 30 —Å–µ–∫—É–Ω–¥)
  - Logged (print results)
  - –°—Ä–∞–≤–Ω—ë–Ω —Å expectation
</required>

---

## Examples

<example>
GOOD: Schema –ø–µ—Ä–µ–¥ –≤—ã–≤–æ–¥–∞–º–∏
```python
df = pd.read_parquet("deals.parquet")
print(f"Shape: {df.shape}")  # (1500, 25)
print(f"dtypes:\n{df.dtypes}")
print(f"nulls:\n{df.isnull().sum()}")
# ‚Üí –í–∏–∂—É 50 nulls –≤ manager_id

# Hypothesis: –≠—Ç–∏ –∑–∞–ø–∏—Å–∏ ‚Äî –∞—Ä—Ö–∏–≤–Ω—ã–µ
# Experiment:
archived = df[df['manager_id'].isnull()]['status'].value_counts()
print(archived)
# ‚Üí 48 –∏–∑ 50 ‚Äî status='archived' ‚úÖ
```
</example>

<example type="invalid">
BAD: –í—ã–≤–æ–¥—ã –±–µ–∑ schema
```
–î–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–¥–µ–ª–∫–∞—Ö...
```
–ü—Ä–æ–±–ª–µ–º–∞: –ù–µ—Ç —Ñ–∞–∫—Ç–æ–≤, –Ω–µ—Ç –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤. Confidence -40%.
</example>

---

## Critical Points

<critical>
  ### Cognitive Bias Prevention:
  - ‚ùå Survivorship: –ù–ï —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ N –∑–∞–ø–∏—Å–µ–π
  - ‚ùå Confirmation: –ù–ï –∏—Å–∫–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
  - ‚úÖ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –í–°–ï –¥–∞–Ω–Ω—ã–µ
  - ‚úÖ –ò—Å–∫–∞—Ç—å –û–ü–†–û–í–ï–†–ñ–ï–ù–ò–Ø –≥–∏–ø–æ—Ç–µ–∑—ã
</critical>

### Anti-patterns

- ‚ùå "–î–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç..." ‚Äî –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏
- ‚ùå –°–ª–æ–∂–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ä–∞–∑—É ‚Äî –º–∞–ª–µ–Ω—å–∫–∏–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å–Ω–∞—á–∞–ª–∞
- ‚ùå Skip schema ‚Äî –í–°–ï–ì–î–ê –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å dtypes, head, nunique

---

**–í–µ—Ä—Å–∏—è:** 1.1
**–°–≤—è–∑–∞–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏:** `_base-5wh.mdc`, `_base-confidence.mdc`
